# Clase 31-3: Heterocedasticidad

## Detección de heterocedasticidad

### Métodos informales

-   Ver antecedentes (estudios previos)

-   Método gráfico: graficar residuos en función de las distintas variables predictoras $X_i$

### Métodos formales

-   Prueba de Park

-   Prueba de Glejser

-   Prueba de Goldfeld-Quandt

-   Prueba de Breusch-Pagan-Godfrey

-   Prueba general de White

## Prueba de Park

Supone que $\sigma_i^2$ es algún tipo de función de la variable explicativa $X_i$:

$$
\sigma_i^2 = \sigma^2 x_i^\beta e^{v_i}
$$

$$
\ln \sigma^2_i = \ln \sigma^2 + \beta \ln X_i + v_i
$$

donde $v_i$ es el término de perturbación.

Pero como no conocemos $\sigma^2$, utilizamos la siguiente regresión:

$$
\ln \hat{u}_i^2 = \ln \sigma^2 + \beta \ln X_i + v_i
$$ {#eq-park}

::: callout-note
## Hipótesis de las pruebas de heterocedasticidad
:::

::: callout-note
Todas las pruebas tienen la siguiente estructura:

-   $H_0:$ Hay heterocedasticidad.

-   $H_1:$ No hay heterocedasticidad.

En términos del valor p:

Si $p > \alpha \implies$ no rechazo $H_0$
:::

::: callout-caution
## Limitación

Existe la posibilidad de que $v_i$ no satisfaga los supuestos de mínimos cuadrados ordinarios y puede ser en sí mismo heterocedástico. Por lo tanto, la prueba de Park se suele usar como método exploratorio.
:::

### Prueba de Park en R

``` r
datos(residuos2 <- datos$residuos^2)
head(datos)
```

Modelo auxiliar de la forma @eq-park

``` r
attach(datos)
modelo_auxiliar_park <- lm(log(residuos2) ~ log(ingresos))
summary(modelo_auxiliar_park)
```

$$
p_\text{valor} (\beta) \approx 0 \implies RH_0
$$

Por lo tanto, la prueba muestra la presencia de heterocedasticidad.

## Prueba de Glejser

Prueba similar a la de Park, con la diferencia que en vez de trabajar con los residuos al cuadrado, trabaja con éstos en valores absolutos. Es una prueba útil en muestras grandes, no en muestras pequeñas ($<30$).

$$
|\hat{u}_i| = \beta_1 + \beta_2 X_i + v_i
$$ {#eq-glejser-1}

$$
|\hat{u}_i| = \beta_1 + \beta_2 \sqrt{X_i} + v_i
$$ {#eq-glejser-2}

$$
|\hat{u}_i| = \beta_1 + \beta_2 \frac{1}{\sqrt{X_i}} + v_i
$$ {#eq-glejser-3}

$$
|\hat{u}_i| = \sqrt{\beta_1 + \beta_2 X_i} + v_i
$$ {#eq-glejser-4}

$$
|\hat{u}_i| = \sqrt{\beta_1 + \beta_2 X_i^2} + v_i
$$ {#eq-glejser-5}

### Prueba de Glejser en R

Agregamos una columna con los valores absolutos de los residuos `abs_residuos`:

``` r
datos$abs_residuos <- abs(datos$residuos)
head(datos)
```

Modelo auxiliar de la forma @eq-glejser-1 :

``` r
attach(datos)
modelo_auxiliar_glejser1 <- lm(abs_residuos ~ Ingreso)
summary(modelo_auxiliar_glejser1)
```

$$
p_\text{valor} (\beta) \approx 0 \implies RH_0
$$

Al igual que con la muestra de Park, la prueba arroja existencia de heterocedasticidad.

Modelo auxiliar de la forma @eq-glejser-2 :

``` r
attach(datos)
modelo_auxiliar_glejser2 <- lm(abs_residuos ~ sqrt(Ingresos))
summary(modelo_auxiliar_glejser2)
```

Que también arroja un P valor cercano a cero, por lo que también rechazamos la hipótesis nula y concluimos que hay heterocedasticidad.

## Prueba de Spearman

El desarrollo es medio engorroso así que sólo escribo el comando en R:

``` r
attach(datos)
cor(abs_residuos, Ingresos, method= "spearman")
```

También se puede usar la función `cor.test` con la misma sintaxis.

::: callout-note
No requiere el supuesto de normalidad para los residuos.
:::

## Prueba de Goldfeld-Quandt

Supone que la varianza $\sigma^2$ está relacionada *positivamente* con *una* variable explicativa del modelo:

$$
\sigma_i^2 = \sigma^2 X_i^2
$$

Procedimiento:

1.  Ordenar las observaciones de menor a mayor en función de $X$.
2.  Omitir las $c$ observaciones centrales, las observaciones restantes se dividen en dos grupos, cada uno con $(n-c)/2$ observaciones.
3.  Ajustar con MCO *separados* los subconjuntos definitos y obtener las respectivas sumas de cuadrados residuales $\hat{u}^2$

::: callout-note
## Valor de c (revisar)

-   El valor $c$ óptimo se encuentra entre el 10 y el 20% del tamaño de la muestra.

-   Se puede hacer una simulación tipo Montecarlo para encontrar dicho valor óptimo.
:::

### Prueba de Goldfeld-Quandt en R

Paso 1: ordenar las observaciones de menor a mayor en función de $X$:

``` r
datos_ordenados <- datos[order(datos$Ingreso)]
head(datos_ordenados)
```

Paso 2: omitir $c$ observaciones centrales

``` r
n = nrow(datos_ordenados) #n=100
c <- 16
n1 = n/2-(c/2)
n-(n1+c)

datos1 <- datos_ordenados[1:n1, ]
datos2 <- datos_ordenados[(n1+c+1):n, ]
```

Paso 3: ajustar con MCO separados los subconjuntos definidos y obtener los cuadrados residuales

``` r
attach(datos1)
modelo1 = lm(Gasto_Educacion ~ Ingreso)

attach(datos2)
modelo2 = lm(Gasto_Educacion ~ Ingreso)

anova(modelo1)
anova(modelo2)
```

``` r
scr1 = sum(modelo1$residuals^2)
scr1
scr2 = sum(modelo2$residuals^2)
scr2
```

``` r
lambda = (scr2/(n1-2))/(scr1/(n1-2))
lambda
p_valor = 1 - pf)(lambda, (n1-2), (n1-2))
p_valor
```

## Prueba de Breusch-Pagan-Goldfrey

Paso 1: estimar el modelo de regresión lineal simple y obtener los residuos.

Paso 2: obtener la varianza máximo-verosímil:

$$
\overset{\sim}{\sigma}^2 = \frac{\sum \hat{u}^2_i}{n}
$$

::: callout-note
Comparar con estimador de MCO:

$$
\sigma^2 = \frac{\sum \hat{u}^2_i}{n-k}
$$

Los grados de libertad lo vuelven un estimador insesgado, a diferencia de $\overset{\sim}{\sigma}$
:::

Paso 3: construir las variables $p_i$:

$$
p_i = \frac{\hat{u}^2_i}{\overset{\sim}{\sigma}^2}
$$

Paso 4: hacer la regresión:

$$
p_i = \alpha_1 + \alpha_2 Z_{2i} + \dots + \alpha_m Z_{mi} + v_i
$$

::: callout-note
Lo más probable es que las $Z$ sean las propias $X$ del modelo.
:::

Paso 5: obtener la suma de cuadrados explicada y definir:

$$
\Theta = \frac{1}{2} SCE_{xp}
$$
